"""
The Car Counter Project is a computer vision-based system
designed to automatically detect and count vehicles as they
pass through a defined area in a video feed. Using object
detection and tracking, the system monitors moving cars and
counts each one that crosses a virtual line. This project is useful
for traffic analysis, smart city infrastructure, and vehicle flow monitoring.

Comments included for the userâ€™s better understanding.
"""

from pydoc import classname
from cv2 import VideoCapture # For capturing video input from camera or file
from ultralytics import YOLO # YOLO object detection model from the Ultralytics library
import cv2                   # OpenCV: Image and video processing
import cvzone                # Simplifies OpenCV tasks like drawing and UI components
import math                  # For mathematical operations
from sort import *           # Imports SORT tracking algorithm classes/functions for object tracking


cap = cv2.VideoCapture("../videos/car.mp4")    # Load video from file for frame-by-frame processing
model = YOLO("../Yolo-Weights/yolov8m.pt")     # Load the YOLOv8n object detection model with pre-trained weights

# List of COCO dataset class names that the YOLO model can detect in images or video frames.
classNames = [
    'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck',
    'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench',
    'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra',
    'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',
    'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',
    'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup',
    'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',
    'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',
    'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse',
    'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',
    'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',
    'toothbrush'
]

#Masking focused region
mask = cv2.imread("mask.png")

#TRACKING
tracker = Sort(max_age=20, min_hits=3, iou_threshold=0.3)   # Initialize SORT tracker with tracking parameters
limits = [400,297,673,297]                                  # Define coordinates for the counting line
totalCount = []                                             # List to store unique object IDs that crossed the line

while True:
    Success, img = cap.read()                       # Read the next video frame
    imgRegion = cv2.bitwise_and(img,mask)           # Apply mask to focus detection on specific region
    img = cv2.resize(img, (1280, 720))        # Resize frame to match expected dimensions
    results = model(imgRegion ,stream = True)       #stream=True use generators making it more efficient. Generators - special iterators that yield items one at a time
    detections= np.empty((0,5))                     # Initialize empty array for storing bounding boxes with confidence scores

    for r in results:
        boxes = r.boxes                             # Get all detected bounding boxes from the result
        for box in boxes:

            #Bounding Box
            x1, y1, x2, y2 = box.xyxy[0]
            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)
            #cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 200), 3)
            #print(x1, y1, x2, y2)
            w, h = x2-x1,y2-y1

            #confidence
            conf = math.ceil((box.conf[0]*100))/100     # Round confidence score to 2 decimal places

            #Class Name
            cls = int(box.cls[0])
            currentClass = classNames[cls]

            if currentClass == "car" or currentClass == "truck" or currentClass == "bus"\
                    or currentClass == "motorbike" and conf >0.3:       # Filter for vehicle types with confidence above threshold
                currentArray = np.array([x1,y1,x2,y2,conf])             # Create detection array
                detections = np.vstack((detections, currentArray))      # Append to detections array


    resultsTracker = tracker.update(detections)                         # Update SORT tracker and receive updated bounding boxes with track IDs

    # Draw red lines on the frame to visually mark the counting boundaries.
    cv2.line(img,(limits[0], limits[1]), (limits[2], limits[3]),(0,0,255),5)

    for result in resultsTracker:
        x1, y1, x2, y2, id = result                                                       # Extract bounding box coordinates and track ID
        x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)
        print(result)
        w, h = x2 - x1, y2 - y1                                                           # Calculate width and height of bounding box
        cvzone.cornerRect(img, (x1, y1, w, h), l=9, rt=2, colorR=[255,0,255])             # Draw rectangle around object
        cvzone.putTextRect(img, f'{int(id)}', (max(0, x1), max(35, y1)),                  # Display track ID near the box
                        scale=2, thickness=3, offset=10)

        cx, cy =x1+w//2, y1+h//2                                                          # Compute center coordinates of the bounding box
        cv2.circle(img,(cx,cy),5,(255,0,255), cv2.FILLED)                                 # Draw filled circle at the center point


        if limits[0] <cx< limits[2] and limits[1]-30<cy<limits[1]+20:                     # Check if the detected object's center (cx, cy) is within the horizontal and vertical bounds of the counting line
            if totalCount.count(id)==0:
                totalCount.append(id)
                cv2.line(img, (limits[0], limits[1]), (limits[2], limits[3]), (0, 255, 0), 5)

    # Draw a green line on the image to indicate that this object has been counted
    cv2.putText(img,str(len(totalCount)),(177,100),cv2.FONT_HERSHEY_PLAIN,5,(50,50,255),8)

    cv2.imshow("Image", img)                        # Show the frame in a window
    # cv2.imshow("ImageRegion", imgRegion)          # Uncomment to view the masked image region during processing
    cv2.waitKey(1)

